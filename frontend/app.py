import os
import logging
import sys
import asyncio

import streamlit as st

from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from openai.types.chat.chat_completion_user_message_param import (
    ChatCompletionUserMessageParam,
)
from openai.types.chat.chat_completion_assistant_message_param import (
    ChatCompletionAssistantMessageParam,
)

# --- Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
st.set_page_config(page_title="DXãƒ†ãƒ¼ãƒå®šç¾©æ”¯æ´ã‚¢ãƒ—ãƒª", layout="wide")

from helpers import clear_data_callback, state_init, get_telemetry_data
import multistep_qa

sys.path.append("..")

from utils.api import process_uploaded_file, fetch_dx_tool_suggestions
from utils.schema import PromptType


logger = logging.getLogger("TalkToMyUseCase")




async def handle_first_question(combined_input) -> None:
    # æ–°ã—ã„ä¼šè©±ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸçŠ¶æ…‹ã‚’è¾æ›¸ã§å®šç¾©
    session_first_question = {
        "chat_history": [
            ChatCompletionUserMessageParam(role="user",
                                        content=combined_input)
            ],
        "conversation_stage": "PROCESSING_INITIAL",
        "questions_asked_flag": False,
        "ai_questions": [],
        "user_answers": {},
        "dx_solution": None,
        "question_counter": 1  # åˆæœŸè³ªå•ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
    }
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ä¸€æ‹¬æ›´æ–°
    for key, value in session_first_question.items():
        st.session_state[key] = value
    logger.info("Session state has been updated by first question.")


async def handle_single_step_ai_response(ai_response: dict) -> None:
    """
    AIã‹ã‚‰ã®å¿œç­”ã‚’å‡¦ç†ã—ã€é©åˆ‡ãªã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’è¨­å®šã—ã¾ã™ã€‚
    Args:
        ai_response: AIã‹ã‚‰ã®å¿œç­”è¾æ›¸
    """
    if not ai_response or ai_response.get("type") == "error":
        st.warning("AIã¨ã®é€šä¿¡ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚„å…¥åŠ›å†…å®¹ã‚’ç¢ºèªã—ã€ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        st.session_state.conversation_stage = "INITIAL_INPUT"
        st.rerun()
        return

    # AIã®å¿œç­”ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.append(
        ChatCompletionAssistantMessageParam(
            role="assistant",
            content=ai_response["message"]
        )
    )
    
    response_type = ai_response["type"]
    
    if response_type == "questions":
        _handle_questions_response(ai_response)
    elif response_type == "solution":
        _handle_solution_response(ai_response)
    else:
        st.error(f"äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ—: {response_type}")
        st.session_state.conversation_stage = "INITIAL_INPUT"
        st.rerun()

def _handle_questions_response(ai_response: dict) -> None:
    """è³ªå•ã‚¿ã‚¤ãƒ—ã®å¿œç­”ã‚’å‡¦ç†ã—ã¾ã™ã€‚"""
    # è¿½åŠ ã®è³ªå•ãŒå¿…è¦ãªå ´åˆï¼ˆè³ªå•å›æ•°ãŒä¸Šé™æœªæº€ï¼‰
    if st.session_state.question_counter >= 5:
        # è³ªå•å›æ•°ãŒä¸Šé™ã«é”ã—ãŸå ´åˆã¯å¼·åˆ¶çš„ã«è§£æ±ºç­–ã‚’è¡¨ç¤º
        st.warning("è³ªå•å›æ•°ãŒä¸Šé™ã«é”ã—ã¾ã—ãŸã€‚ç¾æ™‚ç‚¹ã§ã®æœ€å–„ã®ææ¡ˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
        st.session_state.dx_solution = {
            "tools": ai_response.get("tools", [ai_response.get("tool", "åˆ©ç”¨å¯èƒ½ãªæœ€é©ãªDXãƒ„ãƒ¼ãƒ«")]),
            "primary_tool": ai_response.get("primary_tool", ai_response.get("tool", "åˆ©ç”¨å¯èƒ½ãªæœ€é©ãªDXãƒ„ãƒ¼ãƒ«")),
            "tool_combinations": ai_response.get("tool_combinations", [{
                "tool": ai_response.get("tool", "åˆ©ç”¨å¯èƒ½ãªæœ€é©ãªDXãƒ„ãƒ¼ãƒ«"),
                "purpose": "ä¸»è¦ãªè§£æ±ºæ‰‹æ®µ",
                "todos": ai_response.get("todos", ["ç¾æ™‚ç‚¹ã§è€ƒãˆã‚‰ã‚Œã‚‹æœ€é©ãªToDo"])
                }]),
            "todos": ai_response.get("todos", ["ç¾æ™‚ç‚¹ã§è€ƒãˆã‚‰ã‚Œã‚‹æœ€é©ãªToDo"]),
            "message": "è³ªå•å›æ•°ã®åˆ¶é™ã«é”ã—ãŸãŸã‚ã€é™ã‚‰ã‚ŒãŸæƒ…å ±ã«åŸºã¥ãææ¡ˆã¨ãªã£ã¦ã„ã¾ã™: " + ai_response["message"]
            }
        st.session_state.conversation_stage = "SHOWING_SOLUTION"
    else:
        st.session_state.ai_questions = ai_response["questions"]
    
        # AIã‹ã‚‰ã®è³ªå•ã‚‚å€‹åˆ¥ã«å±¥æ­´ã«è¿½åŠ 
        for q_text in st.session_state.ai_questions:
            st.session_state.chat_history.append(
                ChatCompletionAssistantMessageParam(
                    role="assistant",
                    content=f"è³ªå•: {q_text}"
                )
            )
    
        st.session_state.conversation_stage = "AWAITING_ANSWERS"
        st.session_state.questions_asked_flag = True
    st.rerun()

def _handle_solution_response(ai_response: dict) -> None:
    """è§£æ±ºç­–ã‚¿ã‚¤ãƒ—ã®å¿œç­”ã‚’å‡¦ç†ã—ã¾ã™ã€‚"""
    st.session_state.dx_solution = ai_response
    st.session_state.conversation_stage = "SHOWING_SOLUTION"
    st.rerun()

async def handle_user_answers_form() -> None:
    """
    AIã‹ã‚‰ã®è³ªå•ã«å¯¾ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ãƒ•ã‚©ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã¾ã™ã€‚
    """
    temp_answers = {}
    for i, question_text in enumerate(st.session_state.ai_questions):
        temp_answers[question_text] = st.text_area(
            question_text,
            key=f"answer_q_openai_{i}",
            height=100
        )

    submitted = st.form_submit_button("å›ç­”ã‚’é€ä¿¡ã™ã‚‹")
    if submitted:
        _process_user_answers(temp_answers)

def _process_user_answers(temp_answers: dict) -> None:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‚’æ¤œè¨¼ã—ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¾ã™ã€‚
    
    Args:
        temp_answers: è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’å«ã‚€è¾æ›¸
    """
    all_answered = True
    user_responses_for_history = []
    
    # å›ç­”ã®æ¤œè¨¼ã¨ãƒãƒ£ãƒƒãƒˆå±¥æ­´ç”¨ã®å¿œç­”ä½œæˆ
    for q_text, ans_text in temp_answers.items():
        if not ans_text:
            all_answered = False
            st.warning(f"è³ªå•ã€Œ{q_text}ã€ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            break
        user_responses_for_history.append(
            ChatCompletionUserMessageParam(
                role="user",
                content=f"(è³ªå•ã€Œ{q_text}ã€ã¸ã®å›ç­”) {ans_text}"
            )
        )
    
    # ã™ã¹ã¦ã®è³ªå•ã«å›ç­”ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å‡¦ç†ã‚’é€²ã‚ã‚‹
    if all_answered:
        _update_session_with_answers(temp_answers, user_responses_for_history)

def _update_session_with_answers(temp_answers: dict, user_responses_for_history: list) -> None:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¾ã™ã€‚
    
    Args:
        temp_answers: è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’å«ã‚€è¾æ›¸
        user_responses_for_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ ã™ã‚‹å¿œç­”ã®ãƒªã‚¹ãƒˆ
    """
    st.session_state.user_answers = temp_answers
    st.session_state.chat_history.extend(user_responses_for_history)
    st.session_state.conversation_stage = "PROCESSING_INITIAL"
    st.session_state.question_counter += 1  # è³ªå•ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ
    st.rerun()

def update_checkbox_state_descriptions():
    st.session_state.use_tools_and_descriptions = st.session_state.use_tools_and_descriptions_key

def update_checkbox_state_llms():
    st.session_state.use_multiple_system_prompts = st.session_state.use_multiple_system_prompts_key


async def main():
    """
    ãƒ¡ã‚¤ãƒ³ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    st.title("DXãƒ†ãƒ¼ãƒå®šç¾©æ”¯æ´ã‚¢ãƒ—ãƒª ğŸ’¡")
    st.caption("ãµã‚ã£ã¨ã—ãŸã€Œã‚„ã‚ŠãŸã„ã“ã¨ã€ã‹ã‚‰ã€DXã®ãƒ†ãƒ¼ãƒã¨ToDoã‚’å…·ä½“åŒ–ã—ã¾ã™ã€‚(OpenAI APIé€£æºç‰ˆ)")
    await state_init()


    st.sidebar.checkbox("èª¬æ˜ã®ä»˜ä¸ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹",
                        key="use_tools_and_descriptions_key",
                        on_change=update_checkbox_state_descriptions,
                        value=True)
    if st.session_state.use_tools_and_descriptions:
        st.sidebar.checkbox("è¤‡æ•°ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ©ç”¨ã—ã¦è¿½åŠ ã®è³ªå•ãŒå¿…è¦ã‹åˆ¤æ–­ã•ã›ã‚‹",
                            key="use_multiple_system_prompts_key",
                            on_change=update_checkbox_state_llms,
                            value=True)


    # --- ã‚¹ãƒ†ãƒƒãƒ—1: ã‚„ã‚ŠãŸã„ã“ã¨ã®å…¥åŠ›ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.header("ã€Œã‚„ã‚ŠãŸã„ã“ã¨ã€ã‚’æ•™ãˆã¦ãã ã•ã„")
    initial_user_request = st.text_area(
        "ä¾‹: å–¶æ¥­éƒ¨é–€ã®å ±å‘Šæ›¸ä½œæˆæ¥­å‹™ã‚’åŠ¹ç‡åŒ–ã—ãŸã„ã€é¡§å®¢ã®è§£ç´„ç‡ã‚’äºˆæ¸¬ã—ãŸã„ã€æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ãŸãã•ã‚“å‡ºã—ãŸã„...",
        key="initial_request_input_openai",
        height=100,
        value=st.session_state.user_request_buffer
        )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
    st.subheader("å‚è€ƒè³‡æ–™ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    st.caption("ç¾çŠ¶ã®æ¥­å‹™ã‚„ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")

    with st.expander("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹", expanded=True):
        uploaded_file = st.file_uploader(
            "Excelã€CSVã€PowerPointã€Wordæ–‡æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™",
            type=["xlsx", "xls", "csv", "pptx", "docx"],
            key=st.session_state.file_uploader_key,
        )

        if uploaded_file:
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            with st.spinner(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_file.name}ã€ã‚’å‡¦ç†ä¸­..."):
                file_info = process_uploaded_file(uploaded_file)
                st.session_state.uploaded_files[uploaded_file.name] = file_info
                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{uploaded_file.name}ã€ãŒæ­£å¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
                st.info(file_info["summary"])

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
        if st.session_state.uploaded_files:
            st.subheader("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«:")
            for filename, file_info in st.session_state.uploaded_files.items():
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.write(f"ğŸ“„ {filename}")
                    st.caption(file_info["summary"])
                with col2:
                    if st.button("å‰Šé™¤", key=f"delete_{filename}"):
                        del st.session_state.uploaded_files[filename]
                        st.rerun()

    # é€ä¿¡ãƒœã‚¿ãƒ³
    if st.button("AIã«ç›¸è«‡ã™ã‚‹", 
                 key="submit_initial_request_openai"):
        if initial_user_request:
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’è¿½åŠ ã—ãŸå†…å®¹ã‚’ä½œæˆ
            file_context = ""
            if st.session_state.uploaded_files:
                file_context = "\n\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±:\n"
                for filename, file_info in st.session_state.uploaded_files.items():
                    file_context += f"- {filename}: {file_info['summary']}\n"

            combined_input = initial_user_request + file_context

            st.session_state.user_request = combined_input
            st.session_state.user_request_ = initial_user_request  # å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã ã‘ã‚’ä¿æŒ
            st.session_state.telemetry_json = await get_telemetry_data()
            print(st.session_state.telemetry_json)
            
            await handle_first_question(combined_input)
            st.rerun()
        else:
            st.warning("ã€Œã‚„ã‚ŠãŸã„ã“ã¨ã€ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # --- AIã«ã‚ˆã‚‹åˆ†æã¨å¿œç­”å‡¦ç† ---
    if st.session_state.conversation_stage == PromptType.DECISION:
        with st.spinner("AIãŒåˆ†æä¸­ã§ã™...ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"):
            print(st.session_state.chat_history)
            print(st.session_state.conversation_stage)
            if st.session_state.use_multiple_system_prompts:
                prompt_type=PromptType.DECISION
            else:
                prompt_type=None
            ai_response = await fetch_dx_tool_suggestions(st.session_state.chat_history,
                                                          st.session_state.use_tools_and_descriptions,
                                                          telemetry_json=st.session_state.telemetry_json,
                                                          prompt_type=prompt_type)
            print(st.session_state.conversation_stage)
            # AIã‹ã‚‰ã®å¿œç­”ã‚’å‡¦ç†
            response_type = await handle_single_step_ai_response(ai_response)
            st.session_state.conversation_stage = response_type

    # --- ã‚¹ãƒ†ãƒƒãƒ—1.5: AIã‹ã‚‰ã®è¿½åŠ è³ªå•ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­” ---
    if st.session_state.conversation_stage == PromptType.QUESTION and st.session_state.question_counter <= MAX_QUESTION_ROUNDS:
        # æ®‹ã‚Šã®è³ªå•å›æ•°ã‚’è¡¨ç¤º
        ai_response = await fetch_dx_tool_suggestions(st.session_state.chat_history,
                                                          st.session_state.use_tools_and_descriptions,
                                                          telemetry_json=st.session_state.telemetry_json,
                                                          prompt_type=st.session_state.conversation_stage)
        st.info(f"AIã«ã‚ˆã‚‹è³ªå•å›æ•°: {st.session_state.question_counter}å› / æœ€å¤§{MAX_QUESTION_ROUNDS}å›ä¸­")

        # AIã®è³ªå•ã‚’ä¿ƒã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        _handle_questions_response(ai_response) # This function now sets ai_questions and updates conversation_stage
        st.info(st.session_state.chat_history[-1-len(st.session_state.ai_questions)]["content"])

        with st.form(key="answers_form_genai"):
            await handle_user_answers_form()


    # --- ã‚¹ãƒ†ãƒƒãƒ—2: DXãƒ„ãƒ¼ãƒ«ã¨ToDoãƒªã‚¹ãƒˆã®è¡¨ç¤º ---
    if st.session_state.conversation_stage == PromptType.SOLUTION or st.session_state.question_counter > MAX_QUESTION_ROUNDS:
        ai_response = await fetch_dx_tool_suggestions(st.session_state.chat_history,
                                                      st.session_state.use_tools_and_descriptions,
                                                      telemetry_json=st.session_state.telemetry_json,
                                                      prompt_type=st.session_state.conversation_stage)
        st.session_state.dx_solution = ai_response # Set dx_solution here
        st.session_state.conversation_stage = "SHOWING_SOLUTION" # Update stage

    if st.session_state.conversation_stage == "SHOWING_SOLUTION" and st.session_state.dx_solution:
        solution = st.session_state.dx_solution
    
        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if "tools" in solution and solution["tools"] and solution["tools"][0] != "ã‚¨ãƒ©ãƒ¼":
            st.success("DXãƒ†ãƒ¼ãƒã®å®šç¾©ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            st.error("DXãƒ†ãƒ¼ãƒã®å®šç¾©ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        
        # ææ¡ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
        st.markdown(f"*{solution.get('message', '')}*")
    
        # ä¸»è¦ãªDXãƒ„ãƒ¼ãƒ«ã®è¡¨ç¤º
        primary_tool = solution.get('primary_tool', solution.get('tools', ['N/A'])[0] if solution.get('tools') else 'N/A')
        st.subheader(f"ææ¡ˆã™ã‚‹ä¸»è¦DXãƒ„ãƒ¼ãƒ«: **{primary_tool}**")
    
        # è¤‡æ•°ã®DXãƒ„ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹å ´åˆ
        if 'tool_combinations' in solution and solution['tool_combinations']:
            st.subheader("DXãƒ„ãƒ¼ãƒ«ã®çµ„ã¿åˆã‚ã›:")
        
            for i, tool_combo in enumerate(solution['tool_combinations']):
                with st.expander(f"{i+1}. {tool_combo.get('tool', 'N/A')} - {tool_combo.get('purpose', '')}"):
                    st.markdown(f"**å½¹å‰²**: {tool_combo.get('purpose', 'N/A')}")
                    st.markdown("**é–¢é€£ã™ã‚‹ToDoãƒªã‚¹ãƒˆ**:")
                    tool_todos = tool_combo.get('todos', [])
                    if tool_todos:
                        for j, todo_text in enumerate(tool_todos):
                            st.checkbox(f"{todo_text}", key=f"todo_combo_{i}_{j}")
                    else:
                        st.write("ã“ã®ãƒ„ãƒ¼ãƒ«ã«é–¢é€£ã™ã‚‹ToDoãƒªã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    
        # å…¨ä½“çš„ãªToDoãƒªã‚¹ãƒˆã®è¡¨ç¤º
        st.subheader("å…¨ä½“çš„ãªæ¨å¥¨ToDoãƒªã‚¹ãƒˆ:")
        todos = solution.get('todos', [])
        if todos:
            for i, todo_text in enumerate(todos):
                st.checkbox(f"{todo_text}", key=f"todo_overall_{i}")
        else:
            st.write("ToDoãƒªã‚¹ãƒˆã¯æä¾›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

        # ä½¿ç”¨ã•ã‚Œã‚‹DXãƒ„ãƒ¼ãƒ«ä¸€è¦§ã®è¡¨ç¤º
        if 'tools' in solution and len(solution['tools']) > 1:
            st.subheader("ä½¿ç”¨ã™ã‚‹DXãƒ„ãƒ¼ãƒ«ã®ä¸€è¦§:")
            tool_list_html = ""
            for tool in solution['tools']:
                if tool == primary_tool:
                    tool_list_html += f"- **{tool}** (ä¸»è¦ãƒ„ãƒ¼ãƒ«)\n"
                else:
                    tool_list_html += f"- {tool}\n"
            st.markdown(tool_list_html)

        st.markdown("---")

        # ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
        if st.button("ã‚‚ã†ä¸€åº¦ã€æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™", 
                     key="restart_process_openai",
                     on_click=clear_data_callback):
            st.rerun()


    # --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º (ã‚µã‚¤ãƒ‰ãƒãƒ¼) ---
    if st.sidebar.checkbox("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹", key="show_chat_history_openai", value=True):
        st.sidebar.subheader("ä¼šè©±ã®å±¥æ­´")
        if not st.session_state.get('chat_history', []):
            st.sidebar.write("ã¾ã ä¼šè©±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        for entry in st.session_state.get('chat_history', []):
            with st.sidebar.chat_message(entry["role"]):
                st.markdown(entry["content"])

    st.sidebar.markdown("---")
    st.sidebar.header("ã‚¢ãƒ—ãƒªæ§‹æƒ³ã«ã¤ã„ã¦")
    st.sidebar.markdown("""
    ã“ã®ã‚¢ãƒ—ãƒªã¯AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’åˆ©ç”¨ã—ã¦AIã¨ã®å¯¾è©±ã‚’è¡Œãªã£ã¦ã„ã¾ã™ã€‚
    å®Ÿç¾ã—ãŸã„ã“ã¨ã«å¯¾ã—ã¦DXã®ã©ã‚“ãªæŠ€è¡“ã§è§£æ±ºã§ããã†ã‹åˆ¤æ–­ã—ã¦ãã‚Œã¾ã™ã€‚
    """)
if __name__ == "__main__":
    if os.environ.get("MULTISTEP", "False") == "True":
        asyncio.run(multistep_qa.main())
    else:
        asyncio.run(main())

